%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGES

\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[catalan]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage{array}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{ragged2e}
\usepackage{subfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
\usepackage{float}
\usepackage{chngcntr}
\usepackage{ragged2e}
\usepackage{multirow}
\usepackage{vmargin}
\usepackage{hyperref}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{bigints}
\usepackage{listings}
\usepackage{xcolor,colortbl}
\usepackage[hidelinks]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COLORS

\definecolor{bluebell}{rgb}{0.64, 0.64, 0.82}
\definecolor{atomictangerine}{rgb}{1.0, 0.6, 0.4}
\definecolor{applegreen}{rgb}{0.55, 0.71, 0.0}
\definecolor{frenchblue}{rgb}{0.0, 0.45, 0.73}
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{darkpastelblue}{rgb}{0.47, 0.62, 0.8}
\definecolor{navy}{rgb}{0,0,128}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{amaranth}{rgb}{0.9, 0.17, 0.31}
\definecolor{GRAY}{rgb}{0.75, 0.75, 0.75}
\definecolor{deepfuchsia}{rgb}{0.76, 0.33, 0.76}
\definecolor{deepmagenta}{rgb}{0.8, 0.0, 0.8}
\definecolor{funcblue}{rgb}{0.36, 0.57, 0.9}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SETTINGS

% COLOR LINK/REFERENCES
\hypersetup{colorlinks=true,
            linkcolor=blue,
            filecolor=magenta,      
            urlcolor=blue,
            }

% PAGE'S STYLE OF INITIAL PAGES
\fancypagestyle{plain}{
    \lhead[]{} % left part of the header
    \rhead[]{} % right part of the header
    \fancyfoot[]{} % borrow the foot counter page
    \lfoot[]{} % left part of the footpage
    \rfoot[]{\thepage} % right part of the footpage
    \renewcommand{\headrulewidth}{0pt} % separator line of the header
    \renewcommand{\footrulewidth}{0.5pt} % separator line of the footpage
}

% PAGE'S STYLE FOR THE THESIS
\fancypagestyle{extended_plain}{
    \lhead[]{Gerard Lahuerta Martín} % left part of the header
    \rhead[]{\leftmark} % right part of the header
    \fancyfoot[]{} % borrow the foot counter page
    \lfoot[]{} % left part of the footpage
    \rfoot[]{\thepage} % right part of the footpage
    \renewcommand{\headrulewidth}{0.5pt} % separator line of the header
    \renewcommand{\footrulewidth}{0.5pt} % separator line of the footpage
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CODE STYLE


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT

\begin{document}
\pagestyle{plain}

% TITLE
\begin{titlepage}
    \centering
    {\includegraphics[width=0.4\textwidth]{PORTADA/UAB.png}\par}
    \vspace{1cm}
    {\bfseries\LARGE \href{https://www.uab.cat/}{Universitat Autònoma de Barcelona} \par}
    \vspace{2cm}
    {\Large Degree Thesis \par}
    \vspace{2em}
    \hline
    \vspace{1cm}
    {\scshape\Huge \textbf{Improvements of Deterministic Processes through Neural Networks} \par}
    \vspace{1cm}
    \hline
    \vspace{2cm}
    {\Large
    \begin{figure}[h]
    \hspace{-6.5em}
        \begin{minipage}{10cm} \centering % Minipagina para la tabla. 8 cm de ancho
            \begin{tabular}{c}
                Author: \\
                \href{https://github.com/Gerard-Lahuerta}{Gerard Lahuerta Martín}
            \end{tabular}
        \end{minipage}
        \begin{minipage}{10cm} \centering % Minipagina para la tabla. 8 cm de ancho
            \begin{tabular}{c}
                Supervisor: \\
                \href{https://mat.uab.cat/~alseda/}{Dr. Lluís Alsedà Soler}
            \end{tabular}
        \end{minipage}
    \end{figure}\par}
    \vspace{2cm}
    {\scshape 
        \begin{minipage}{10cm} \centering % Minipagina para la tabla. 8 cm de ancho
            A thesis submitted in fulfilment of the requirements
            for the degree of Computational Mathematics and Data Analytics in the
        \end{minipage} 
    \par} 
    \vspace{2em}
    {\scshape\Large \href{https://www.uab.cat/ciencies}{science faculty} \par}
    \vfill
    {\Large June 2024 \par}
\end{titlepage}


\justifying
\newpage
\textcolor{white}{a}
\newpage 

% PRE SECTIONS
\addcontentsline{toc}{section}{Declaration of Authorship}
\section*{Declaration of Authorship}
\newpage
\addcontentsline{toc}{section}{Abstract}
\section*{Abstract}
\newpage
\addcontentsline{toc}{section}{Acknowledgements}
\section*{Acknowledgements}
I would like to express my gratitude to my family for their support throughout my thesis, degree, and my entire life.\\
\textcolor{white}{a}\\
I am especially grateful to my mother, Maria Montserrat, for her encouragement and guidance in facing the challenges of this thesis and my career.\\
\textcolor{white}{a}\\
Thanks to all the peoples that treatme as I was part of their family.\\
\textcolor{white}{a}\\
I also appreciate the encouragement and support of my colleagues.\\
\textcolor{white}{a}\\
Finally, I want to give a special mention to Dr. Lluís Alsedà for guiding me through the thesis and being an inspiration.\\
\textcolor{white}{a}\\
Thank you all for everything. 
\newpage

% CONTENTS
\tableofcontents
% \cleardoublepage
% \addcontentsline{}{section}{}


\newpage
\addcontentsline{toc}{section}{Preface}
\section*{Preface}


\newpage
\addcontentsline{toc}{section}{Introducción}
\section*{Introducción}
\newpage

\newpage \pagestyle{extended_plain}
\section{Neural Networks} 
Before starting programming and testing a Neural Network is necessary to understand how Neural Networks work.\\
\textcolor{white}{a}\\
A neural Network is made of individual and independent elements connected between them, passing and managing the information through the network formed.\\
\textcolor{white}{a}\\
In this thesis we will focus on one of the simplest networks, a multilayer perceptron, to test the different methods of optimization.

\subsection{Multilayer Perceptron and Perceptron neuron}
One of the simple Neural Networks to analyse and program is the Multilayer Perceptron.\\
\textcolor{white}{a}\\
It was first proposed by Frank Rosenblat\footnote{Frank Rosenblat, psychologist and father of deep learning, check its \href{https://en.wikipedia.org/wiki/Frank_Rosenblat}{biografy}.} in 1958 (nevertheless its approach did not learn either produce accurate results).\\
\textcolor{white}{a}\\
This Neural Network is formed by elements (the artificial neurons) called \textit{Perceptrons} (which gives its name to this network). This neuron is formed by input, weight and activation functions.
\begin{figure}[h]
    \begin{minipage}{9cm}
       \begin{center}
           \includegraphics[width = 1 \textwidth]{Neural_Network/perceptron_schema.png}
           \caption{Schema of the Perceptron neuron}
       \end{center} 
    \end{minipage}
    \hspace{1em}
    \begin{minipage}{5cm}
        The number of inputs that the Perceptron receives is variable but at least needs one input and a biaxe (represented in the picture as the input with value 1).\\
        \textcolor{white}{a}\\
        All the inputs are escalated by a factor (the weights of the Perceptron) and then summed together
    \end{minipage}
\end{figure}\\
Finally, the result of this sum is introduced in the activation function which returns the output of the neuron.\\
\textcolor{white}{a}\\
The traditional activation function used in the Multilayer Perceptron is the Sigmoid: 
$$f(x) = \frac{1}{1+e^{-w\cdot x}},\text{ where: } x,w\in\mathbb{R}^n$$
The objective of the activation function is to obtain how relevant the result of the weighted sum is.\\
\textcolor{white}{a}\\
The use of the sigmoid function as the activation function allows to represent the output of the perception as a probability (because the image of the sigmoid function is $[0,1]$).
\newpage
\hspace{-1.6em}The Multilayer Perceptron is divided into layers with neurons, we can identify three types of layers:
\begin{figure}[h]
    \begin{minipage}{9cm}
        \includegraphics[width = 1 \textwidth]{Neural_Network/Multi-layer-perceptron-MLP-NN-basic-Architecture.png}
        \caption{Schema of the Multilayer perceptron}
    \end{minipage}
    \hspace{1em}
    \begin{minipage}{5cm}
        \begin{itemize}
            \item \textbf{Input Layer}: The initial set of neurons of the Multilayer Perceptron.
            \item \textbf{Output Layer}: The final set of neurons of the Multilayer Perceptron.
            \item \textbf{Hidden Layers}: The set of neurons (in layers) between the input and output layers.
        \end{itemize}
    \end{minipage}
\end{figure}\\
The number of hidden layers depends on the problem and the number of neurons in each layer could not be the same.\\
\textcolor{white}{a}\\
Moreover, mention some vocabulary used in the Thesis in relation to the Neural Networks architectures:
\begin{itemize}
    \item Fully connected: Architecture with all the neurons connected between layers.
    \item Deep: Architecture where the number of layers is huge to perform complex regression or classification tasks.
\end{itemize}
Therefore, we can interpret a Multilayer Perceptron as a weighted sum of activation functions.\\
\textcolor{white}{a}\\
But, why a weighted sum of functions can perform complex tasks like classification or regression?

\subsection{Why Neural Networks works}
Let's imagine that we want to perform regression (for example) on a set of points that exhibit a certain trend, such as a polynomial trend.\\
\textcolor{white}{a}\\
In this scenario, it's important to note that any function can be approximated by an infinite sum of sigmoids:
\begin{equation}
    f(x) \approx \sum_{i = 0}^N a_i\sigma(w^T_i\cdot x + b_i)
\end{equation}
In this context, the function $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ is a continuous function to be approximated, $\sigma:\mathbb{R} \rightarrow \mathbb{R}$ is the sigmoid function, $w_i$ represents the weights of the variable $x\in\mathbb{R}^n$, $b_i$ represents the bias, and the parameter $a_i$ is a vector of size $(m,1)$ containing the weights of the sigmoid function.\\
\textcolor{white}{a}\\
This approximation is known as the \textbf{Universal Approximation Theorem}, and it enables the Multilayer Perceptron to perform both regression and classification tasks.
\subsection{Cost Function}
The most relevant application of the \textbf{Universal Approximation Theorem} in the field of Neural Networks is that it allows us to approximate cost functions, which are functions that evaluate the errors made by the model.\\
This approximation enables the model to learn by minimizing the error.\\
\textcolor{white}{a}\\
The most used cost functions are:
\begin{enumerate}
    \item \textbf{Mean Squared Error}:\\
    The Mean Squared Error (\textit{MSE}) is used in regression tasks. It estimates the error of the model. Its expression is:
    \begin{equation}
        MSE = \frac{1}{2} \sum_{i = 1}^{n} \left( \hat{y_i} - y_i \right)^2
    \end{equation}
    Where $ \hat{y}_i := f(x_i)$ represents the predicted value of the dependent variable $y_i$ to be estimated, and $n$ is the number of samples in the dataset, where $n\in\mathbb{N}$.
    \item \textbf{Cross-Entropy Loss}:\\
    The Cross-Entropy Loss (\textit{CEL}) is used in multi-class classification tasks. It estimates the accuracy of the model. Its expression is:
    \begin{equation}
        CEL = - \frac{1}{n} \sum_{i=1}^n \left(  y_{i} \cdot log(\hat{y_{i}}) \right), \text{ where } \hat{y}_i := f(x_i), n\in \mathbb{N}
    \end{equation}
     Where $ \hat{y}_i$ is the predicted probability of the \textit{i-th} data classified as the class $y_i$ and $n\in\mathbb{N}$ the number of samples.\\
\end{enumerate}
Despite the existence of these two cost functions, there are more complex functions that are widely used, such as the \textbf{Mean Absolute Error}.\\
\textcolor{white}{a}\\
This minimization process (despite the cost function) is used iteratively to determine the values of the parameters in Neural Networks, regardless of the type of task or network.\\
\textcolor{white}{a}\\
The use of iterative optimizers to obtain the best parameters for a Neural Network arises due to the complexity of finding the coefficients that minimize the cost function.\\ 
Moreover, in some cases, it's possible that such coefficients may not exist or cannot be obtained analytically.\\
\textcolor{white}{a}\\
This is why it's important to use differentiable cost functions in training neural networks, as they allow us to leverage the gradient of the cost function to obtain the optimal set of parameters. However, we will discuss this topic in more detail shortly.

\newpage
\subsection{BackPropagation}
The importance of using differentiable cost functions lies in their compatibility with the BackPropagation training algorithm. This algorithm relies on the derivatives of the cost function to compute the gradients for each neuron, which are then used in gradient descent optimization.\\
\textcolor{white}{a}\\
One widely used example, often employed for educational purposes to practice and understand the algorithm, is the following network:
\vspace{-1em}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 1 \textwidth]{Neural_Network/back-graph.png}
    \vspace{-3em}
    \caption{Example of BackPropagation algorithm in simple graph}
    \label{back-prop-example}
\end{figure}\\
In the example shown in Figure \ref{back-prop-example}, if we want to modify the value of the parameter $a$ based on the output $d$, we can apply the chain rule of derivatives, as demonstrated in this case:
\begin{equation*}
    \frac{\partial D}{\partial a} = \frac{\partial d}{\partial a} + \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a} = c + a = 2a+b
\end{equation*}
This algorithm solves one of the biggest problems in early neural network studies related to their development and training. In practice, when a neural network is used, regardless of the task, the only information available is the input and the output.\\
\textcolor{white}{a}\\
This aspect renders the usage of neural networks with more than 2 layers (with more than 1 hidden layer) unviable.\\
\textcolor{white}{a}\\
The solution to this problem was the \textbf{Back-Propagation} algorithm, which "propagates the effects backwards", through each neuron based on its relevance to the obtained output.\\
\textcolor{white}{a}\\
This solution was first introduced by Frank Rosenblatt in 1962 for training an \textit{MLP} (Multi-Layer Perceptron). It was subsequently adopted and used in the training process of various types of neural networks due to its effectiveness, allowing the network to optimize the parameters of each neuron to minimize the error, as represented by the cost function, through \textbf{gradient descent}.
\newpage 

\begin{thebibliography}{X}
\bibitem{WIKIPEDIA-MLP} \textsc{Wikipedia},
\textit{Multilayer perceptron},\\ \url{https://en.wikipedia.org/wiki/Multilayer_perceptron}
\bibitem{WIKIPEDIA-PERCERPTRON} \textsc{Wikipedia},
\textit{Perceptrón},\\ \url{https://es.wikipedia.org/wiki/Perceptr%C3%B3n}
\bibitem{WIKIPEDIA-ACTIV.FUNCT,} \textsc{Wikipedia},
\textit{Activation function},\\ \url{https://en.wikipedia.org/wiki/Activation_function}
\bibitem{WIKIPEDIA-FRANK} \textsc{Wikipedia},
\textit{Frank Rosenblatt},\\ \url{https://es.wikipedia.org/wiki/Frank_Rosenblatt}
\bibitem{WIKIPEDIA-ACTIVATINGFUNCTION} \textsc{Wikipedia},
\textit{Activating Function},\\ \url{https://en.wikipedia.org/wiki/Activating_function}
\bibitem{WIKIPEDIA-ART.NN} \textsc{Wikipedia},
\textit{Artificial neural network},\\ \url{https://en.wikipedia.org/wiki/Artificial_neural_network}
\bibitem{WIKIPEDIA-NN} \textsc{Wikipedia},
\textit{Neural network},\\ \url{https://en.wikipedia.org/wiki/Neural_network}
\bibitem{WIKIPEDIA-SIGMOID_TEOREM} \textsc{Wikipedia},
\textit{Universal Approximation Theorem},\\ \url{https://en.wikipedia.org/wiki/Universal_approximation_theorem}
\bibitem{WIKIPEDIA-MSE} \textsc{Wikipedia},
\textit{Mean Squared Error},\\ \url{https://en.wikipedia.org/wiki/Mean_squared_error}
\bibitem{WIKIPEDIA-MAE} \textsc{Wikipedia},
\textit{Mean Absolute Error},\\ \url{https://en.wikipedia.org/wiki/Mean_absolute_error}
\bibitem{WIKIPEDIA-CE} \textsc{Wikipedia},
\textit{Cross-Entropy},\\ \url{https://en.wikipedia.org/wiki/Cross-entropy}
\bibitem{WIKIPEDIA-BACKPROP} \textsc{Wikipedia},
\textit{BackPropagation},\\ \url{https://en.wikipedia.org/wiki/Backpropagation}

\bibitem{Dan} \textsc{Dantzig, G.B.} y \textsc{P. Wolfe},
<<Decomposition principle for linear programs>>,
\textit{Operations Research}, \textbf{8}, págs. 101--111, 1960.

\end{thebibliography}
\end{document}
